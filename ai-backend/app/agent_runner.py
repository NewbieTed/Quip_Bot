import json
import logging

from langchain_core.messages import AIMessage
from langgraph.types import interrupt

import app.graph
from langchain_mcp_adapters.client import MultiServerMCPClient
from fastmcp import Client


from pathlib import Path
SYSTEM_PROMPT_PATH = Path("prompts/system_prompt.txt")
# SYSTEM_PROMPT_PATH = Path("app/prompts/system_prompt.txt")

# Import the graph cache from graph module
from app.graph import get_cached_graph

# Configure logger for this module
logger = logging.getLogger(__name__)


def load_system_prompt() -> str:
    return SYSTEM_PROMPT_PATH.read_text(encoding="utf-8").strip()


async def run_agent(member_message: str, member_id: int, channel_id: int):
    logger.info("Starting agent run for member_id=%s, channel_id=%s", member_id, channel_id)
    
    if not isinstance(member_message, str) or not member_message.strip():
        logger.error("Invalid message provided: %s", type(member_message))
        yield "Error: Provided message must be a non-empty string."
        return

    # Get the cached graph (recompiles only if tools changed)
    logger.debug("Retrieving cached graph")
    graph = await get_cached_graph()

    config = {"configurable": {"thread_id": member_id}}
    system_message: str = load_system_prompt()
    messages = [
        {
            "role": "system",
            "content": system_message
        },
        {
            "role": "user",
            "content": member_message.strip()
        }
    ]

    # Create initial state with runtime context
    initial_state = {
        "messages": messages,
        "member_id": member_id,
        "channel_id": channel_id
    }

    last_content = None
    try:
        async for mode, chunk in graph.astream(initial_state, config,
                                               stream_mode=["updates", "messages", "custom"]):

            # print(f"[STREAM MODE] {mode}", flush=True)
            # tools = await mcp_client.list_tools()
            if mode == "messages":
                logger.debug("Received message stream chunk")
                if isinstance(chunk, dict) and "messages" in chunk:
                    partial_message = chunk["messages"][-1].content
                    yield partial_message
                continue
            # if mode == "updates":
            #     print(f"[Updates STREAM] {chunk}", flush=True)
            #     ai_message = (
            #         chunk.get("agent", {})
            #         .get("messages", [{}])[0]
            #     )
            #     if isinstance(chunk, dict) \
            #             and isinstance(ai_message, AIMessage) \
            #             and len(ai_message.tool_calls) > 0 \
            #             and ai_message.tool_calls[0].get("type") == "tool_call":
            #         tool_call = ai_message.tool_calls[0]
            #         tool_name = tool_call["name"]
            #         args = tool_call["args"]
            #         if isinstance(args, str):
            #             args_json = json.loads(args)
            #         else:
            #             args_json = args
            #         print(f"Calling tool: {tool_name} with params {args}", flush=True)
            #         continue
            if mode == "custom":
                if isinstance(chunk, dict):
                    message = chunk.get('progress', chunk)
                else:
                    message = chunk
                logger.debug("Custom stream message: %s", message)
                yield message
                continue
            for value in chunk.values():
                if isinstance(value, dict) and "messages" in value:
                    last_content = value["messages"][-1].content
    except Exception as e:
        logger.exception("Stream error occurred for member_id=%s: %s", member_id, str(e))
        yield f"Error: {str(e)}"

    if last_content:
        logger.info("Agent run completed successfully for member_id=%s", member_id)
        yield last_content
    else:
        logger.warning("No response generated by assistant for member_id=%s", member_id)
        yield "No response generated by the assistant."


