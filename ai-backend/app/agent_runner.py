import json

from langchain_core.messages import AIMessage
from langgraph.types import interrupt

import app.graph
from langchain_mcp_adapters.client import MultiServerMCPClient
from fastmcp import Client


from pathlib import Path
SYSTEM_PROMPT_PATH = Path("prompts/system_prompt.txt")
# SYSTEM_PROMPT_PATH = Path("app/prompts/system_prompt.txt")

async def progress_handler(
    progress: float,
    total: float | None,
    message: str | None
) -> None:
    if total is not None:
        percentage = (progress / total) * 100
        print(f"Progress: {percentage:.1f}% - {message or ''}", flush=True)
    else:
        print(f"Progress: {progress} - {message or ''}", flush=True)

def load_system_prompt() -> str:
    return SYSTEM_PROMPT_PATH.read_text(encoding="utf-8").strip()


async def run_agent(member_message: str, member_id: int, channel_id: int):
    if not isinstance(member_message, str) or not member_message.strip():
        yield "Error: Provided message must be a non-empty string."
        return

    client = MultiServerMCPClient(
        {
            "weather": {
                "url": "http://host.docker.internal:8000/mcp",
                # "url": "http://localhost:8000/mcp",
                "transport": "streamable_http"
            }
        }
    )
    tools = await client.get_tools()
    print(tools, flush=True)
    graph = await app.graph.setup_graph(tools)

    config = {"configurable": {"thread_id": member_id}}
    system_message: str = load_system_prompt()
    system_user_data: str = f"Member ID: {member_id}, \nChannel ID: {channel_id}"
    messages = [
        {
            "role": "system",
            "content": system_message + "\n" + system_user_data
        },
        {
            "role": "user",
            "content": member_message.strip()
        }
    ]

    last_content = None
    try:
        async for mode, chunk in graph.astream({"messages": messages}, config,
                                               stream_mode=["updates", "messages", "custom"]):

            # print(f"[STREAM MODE] {mode}", flush=True)
            # tools = await mcp_client.list_tools()
            if mode == "messages":
                # print(f"[MESSAGE STREAM] {chunk}", flush=True)
                if isinstance(chunk, dict) and "messages" in chunk:
                    partial_message = chunk["messages"][-1].content
                    yield partial_message
                continue
            # if mode == "updates":
            #     print(f"[Updates STREAM] {chunk}", flush=True)
            #     ai_message = (
            #         chunk.get("agent", {})
            #         .get("messages", [{}])[0]
            #     )
            #     if isinstance(chunk, dict) \
            #             and isinstance(ai_message, AIMessage) \
            #             and len(ai_message.tool_calls) > 0 \
            #             and ai_message.tool_calls[0].get("type") == "tool_call":
            #         tool_call = ai_message.tool_calls[0]
            #         tool_name = tool_call["name"]
            #         args = tool_call["args"]
            #         if isinstance(args, str):
            #             args_json = json.loads(args)
            #         else:
            #             args_json = args
            #         print(f"Calling tool: {tool_name} with params {args}", flush=True)
            #         continue
            if mode == "custom":
                if isinstance(chunk, dict):
                    message = chunk.get('progress', chunk)
                else:
                    message = chunk
                print(f"[CUSTOM STREAM] {message}", flush=True)
                yield message
                continue
            for value in chunk.values():
                if isinstance(value, dict) and "messages" in value:
                    last_content = value["messages"][-1].content
    except Exception as e:
        import traceback
        print("[STREAM ERROR]", e, flush=True)
        traceback.print_exc()
        yield f"Error: {str(e)}"

    if last_content:
        yield last_content
    else:
        yield "No response generated by the assistant."


